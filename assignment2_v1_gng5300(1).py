# -*- coding: utf-8 -*-
"""Assignment2_V1_gng5300.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BOxqkTJKG4TK802bYJscffKLrQRy7kcZ

#**Assignment 2 | IAI5101(GNG5300) Winter 2022 | Zain Ur-Rehman | 300207267 | zurre072@uottawa.ca**

Import Libraries
"""

import sklearn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import preprocessing
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler, QuantileTransformer
from datetime import date, time, datetime
from sklearn.neighbors import KNeighborsClassifier
import xgboost as xgb
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import VotingClassifier
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn import tree
from sklearn import svm
from sklearn.utils import shuffle
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report , accuracy_score, confusion_matrix
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, precision_recall_fscore_support
import warnings #avoid warning flash
warnings.filterwarnings('ignore')

"""# **(A)EDA and (B)Feature Engineering**

After uploading heart.csv on google drive, use drive.mount and path to the file.
"""

from google.colab import drive
drive.mount('/content/drive')
print ("\nOnce google drive is mounted and correct file path is provided below,")
print ("it takes about 40 seconds to read the excel file from drive")
df = pd.read_csv("/content/drive/MyDrive/GNG5300/heart.csv") # Takes about 40 seconds to read the file from drive

"""Printing unique values for all features"""

# Numerical features
numerical_features = ["Age","RestingBP","Cholesterol","FastingBS","MaxHR","Oldpeak","HeartDisease"]
# Categorical features
categorical_features = ["Sex","ChestPainType","RestingECG","ExerciseAngina","ST_Slope"]

# Printing unique values in each numerical_feature
for b in numerical_features:
    print("------------------numerical_feature----------------------"+b+": ")
    #print(sorted(df[b].unique()))
    print(df[b].value_counts())

# Printing unique values in each categorical features
for a in categorical_features:
    print("------------------categorical_feature----------------------"+a+": ")
    #print(sorted(df[a].unique()))
    print(df[a].value_counts())

# Print overall unique possible columns
print ("\nOverall unique possible columns:")
print ("--------------------------------")
for column in df.iloc[:,np.r_[1,2,3,4,5,6,7,8,9,10,11]].columns:
    print("{}:\n{}".format(column,df[column].unique()))

"""**Univariate analysis**

Using a histogram, plot a distribution of the numerical values
"""

# Histogram of numerical columns
df.hist(figsize=(16, 20), xlabelsize=8, ylabelsize=8)

"""**Bivariate analysis**

Plot a histogram showing the age against the target variable (positive vs. negative cases)

"""

# A histogram showing the age against the target variable (positive vs. negative cases)
plt.figure(figsize = (20,8))
sns.countplot('Age', hue = 'HeartDisease', data = df)

plt.title('Heart Disease Frequency for Age')
plt.legend(['No Disease','Yes Disease'])

plt.xlabel('Age')
plt.ylabel('Frequency')

plt.show()

"""Compare the median age for male and female using a boxplot

median age positive female cases: 58

median age positive male cases: 57
"""

# Compare the median age for male and female using a boxplot
# Reference: https://stackoverflow.com/questions/45475962/labeling-boxplot-with-median-values
# ['M' 'F'] --> [1 0]
plt.figure(figsize=(10,10))
testPlot = sns.boxplot(x='HeartDisease', y='Age', hue='Sex', data=df)
m1 = df.groupby(['HeartDisease', 'Sex'])['Age'].median().values
mL1 = [str(np.round(s, 2)) for s in m1]

ind = 0
for tick in range(len(testPlot.get_xticklabels())):
    testPlot.text(tick-.2, m1[ind+1]+1, mL1[ind+1],  horizontalalignment='center',  color='w', weight='semibold')
    testPlot.text(tick+.2, m1[ind]+1, mL1[ind], horizontalalignment='center', color='w', weight='semibold')
    ind += 2    
plt.show()

"""**Convert categorical data into numerical data using label encoding**



"""

# Using laberl encoder to convert non-numeric data into numberic data
le = LabelEncoder()
df.Sex = le.fit_transform(df.Sex)
df.ChestPainType = le.fit_transform(df.ChestPainType) 
df.RestingECG = le.fit_transform(df.RestingECG) 
df.ExerciseAngina = le.fit_transform(df.ExerciseAngina) 
df.ST_Slope = le.fit_transform(df.ST_Slope) 
# After label encoding
df.head()

#Sex:
#['M' 'F'] --> [1 0]
#ChestPainType:
#['ATA' 'NAP' 'ASY' 'TA']-->  [1 2 0 3]
#RestingECG:
#['Normal' 'ST' 'LVH'] --> [1 2 0]
#ExerciseAngina:
#['N' 'Y'] --> [0 1]
#ST_Slope:
#['Up' 'Flat' 'Down'] --> [2 1 0]

# Get the actual datapoint that is an outlier
q1 = df.quantile(0.25)
q3 = df.quantile(0.75)
IQR = q3 - q1
(df < (q1 -1.5 * IQR)) | (df >(q3 + 1.5 * IQR))

"""**Multivariate Analysis**

Use a heatmap to check for correlation between predictor variables

These features have positive correlation with HeartDisease:

Age, Sex, RestingBP, FastingBS, ExerciseAngina, Oldpeak

"""

print(df.dtypes)
# All of them numeric i.e. int/float
print(df.info())
fig, ax = plt.subplots(figsize=(15,15))
corrmat=df.corr()
sns.heatmap(corrmat, annot=True);

"""**Check for duplicates & missing values. Drop, if present**"""

#dropping duplicate values - checking if there are any duplicate rows and dropping if any
df=df.drop_duplicates()

# Check for missing values in the data. Using the info() function. All columns are have 918 entries. This means, there is no missing value
print ("\nCheck for missing values in the data. Using the info() function. All columns are have 918 entries. This means, there is no missing value")
print(df.info())
print(df.isnull().sum())

"""**There are some outliers in the dataset, (e.g., 0 cholesterol, negative oldpeak) handle them before building the model**"""

print("Before removing the negative and 0 cholesterol")
print("---------------------------------------------")
print( df['Cholesterol'].value_counts() )
# Get names of indexes for which column Cholesterol has value less than 0 or negative
a = df.index[df['Cholesterol'] < 1.0]
print (a)
df = df.drop(index=a)
print("\nAfter removing the negative and 0 Cholesterol")
print("------------------------------------------------")
print( df['Cholesterol'].value_counts() )

print("Before removing the negative and 0 Oldpeak")
print("---------------------------------------------")
print( df['Oldpeak'].value_counts() )
# Get names of indexes for which column Cholesterol has value less than 0 or negative
a = df.index[df['Oldpeak'] < 0.0]
print (a)
df = df.drop(index=a)
print("\nAfter removing the negative and 0 Oldpeak")
print("------------------------------------------------")
print( df['Oldpeak'].value_counts() )

"""**Check for class imbalance**

There seems no class imbalance. 

For no heart disease, the count is around 380.

For yes heard disease, the count is around 350.

Not much difference so this looks like a balanced dataset.
"""

# Check for class imbalance. 
# There seems no class imbalance. 
# For no heart disease, the count is around 380.
# For yes heard disease, the count is around 350.
# Not much difference so this looks like a balanced dataset.

sns.set_style('whitegrid')
sns.countplot(x='HeartDisease',data=df,palette='RdBu_r')

"""**Scale the data using a standard scaler**

Why standard scaling? Because there are features that have lot of variation and also measured in different units.

So when different units, prefer standard scaling. This will re-scale the values within the same distribution.
"""

# Scale the data using a standard scaler
# Why standard scaling? Because there are features that have lot of variation and also measured in different units.
# So when different units, prefer standard scaling. This will re-scale the values within the same distribution.

columns_to_scale = ['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 'FastingBS', 'RestingECG', 'MaxHR', 'ExerciseAngina', 'Oldpeak', 'ST_Slope']
standardScaler = StandardScaler()
df[columns_to_scale] = standardScaler.fit_transform(df[columns_to_scale])

# Normalization 
# The min-max approach (often called normalization) rescales the feature to a hard and fast range of [0,1] by subtracting the minimum value of the feature then dividing by the range.
#columns_to_standardize = ['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 'FastingBS', 'RestingECG', 'MaxHR', 'ExerciseAngina', 'Oldpeak', 'ST_Slope']
#df[columns_to_standardize] = MinMaxScaler().fit_transform(df[columns_to_standardize])

"""Make X and Y dataframes prior to test train split

Using a train (70%) and test (30%) dataset split

X_train, X_test, y_train, y_test
"""

#X=df.filter(['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 'FastingBS', 'RestingECG', 'MaxHR', 'ExerciseAngina', 'Oldpeak', 'ST_Slope'], axis=1) # Keep it pandas.core.frame.DataFrame

X=df.filter(['Age', 'Sex', 'RestingBP', 'FastingBS', 'ExerciseAngina', 'Oldpeak'], axis=1) 
Y=df.filter(['HeartDisease'], axis=1).squeeze() # Change from pandas.core.frame.Dataframe to pandas.core.series.Series

# train_test_split based on X_CleanDF_correlation (few features in X with higher / positive correlation)
X_train, X_test, y_train, y_test= train_test_split(X,Y,stratify=Y, test_size=0.3,random_state=0)#splitting data in 70% train, 30%test

# stratify=Y. This stratify parameter makes a split so that the proportion of values in the sample produced will be the same as the proportion of values provided to parameter stratify.
# For example, if variable y is a binary categorical variable with values 0 and 1 and there are 25% of zeros and 75% of ones, stratify=y will make sure th

"""# Model Development I

**Ensemble Method**

Following models part of Ensemble method:

(1)KNN_k5_classifier with k=5

(2)SVM_classifier with kernel='rbf'

(3)DecisionTree_classifier_grid with Optimal depth of the decision tree:  max_depth: 1

(4)xgBoost_classifier

Both Ensemble approaches votesoft_classifier and votehard_classifier are explored.

**Other classifiers explored that are not part of Ensemble method**:

DecisionTree_classifier (default without any depth)

gradBoost_classifier

"""

# KNN (k=5)
KNN_k5_classifier = KNeighborsClassifier(n_neighbors=5)
KNN_k5_classifier.fit(X_train, y_train)
y_pred_KNN_k5_classifier = KNN_k5_classifier.predict(X_test)

print("<>================================================================================================<>")
print("<>============================== KNN_k5_classifier RESULTS: ==================================<>")
print("<>================================================================================================<>")
# confusion matrix
# pass truth and prediction in cm function 
# confusion_matrix(truth, prediction)
cm_KNN_k5_classifier = confusion_matrix(y_test, y_pred_KNN_k5_classifier)
accuracy_KNN_k5_classifier = accuracy_score(y_test, y_pred_KNN_k5_classifier)
#accuracy_KNN_k5_classifier = accuracy_score(y_pred_KNN_k5_classifier, y_test)
# cross_val_score
KNN_k5_classifier_Score = cross_val_score(KNN_k5_classifier, X_train, y_train, cv=5)
print("\n<>-----The 5 fold KNN_k5_classifier_Score cross validation: ----------<>")
print(KNN_k5_classifier_Score)
# Getting mean
print("\n<>-----KNN_k5_classifier_Score Mean and Standard Deviation: ----------<>")
print(KNN_k5_classifier_Score.mean(), KNN_k5_classifier_Score.std())
print("\n<>-----KNN_k5_classifier confusion_matrix: ---------------------------<>")
print(cm_KNN_k5_classifier)
print("\n<>-----KNN_k5_classifier accuracy score: -----------------------------------<>")
print(accuracy_KNN_k5_classifier)
print("\n<>-----KNN_k5_classifier Classification report: ----------------------<>")
print(metrics.classification_report(y_test, y_pred_KNN_k5_classifier))
print("\n<>-----KNN_k5_classifier confusion_matrix plot: ----------------------<>")
# Code to plot true positives and true negatives based on confusion matrix
# Reference https://www.stackvidhya.com/plot-confusion-matrix-in-python-and-why/
# Reference https://www.ritchieng.com/machine-learning-evaluate-classification-model/
# True Negatives (TN): Model correctly predicted that HeartDisease 0
# False Negatives (FN): Model incorrectly predicted that HeartDisease 0 
# True Positives (TP): Model correctly predicted that HeartDisease 1
# False Positives (FP): Model incorrectly predicted that HeartDisease 1

group_names = ['True Neg','False Pos','False Neg','True Pos']

group_counts = ["{0:0.0f}".format(value) for value in
                cm_KNN_k5_classifier.flatten()]

group_percentages = ["{0:.2%}".format(value) for value in
                     cm_KNN_k5_classifier.flatten()/np.sum(cm_KNN_k5_classifier)]

labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
          zip(group_names,group_counts,group_percentages)]

labels = np.asarray(labels).reshape(2,2)

ax = sns.heatmap(cm_KNN_k5_classifier, annot=labels, fmt='', cmap='Blues')

ax.set_title('Seaborn Confusion Matrix KNN_k5_classifier with labels\n\n');
ax.set_xlabel('\nPredicted Values')
ax.set_ylabel('Actual Values ');

## Ticket labels - List must be in alphabetical order
ax.xaxis.set_ticklabels(['HeartDisease 0','HeartDisease 1'])
ax.yaxis.set_ticklabels(['HeartDisease 0','HeartDisease 1'])

## Display the visualization of the Confusion Matrix.
plt.show()

#SVM with kernel = rbf
SVM_classifier =  svm.SVC(kernel='rbf', probability=True)
SVM_classifier.fit(X_train, y_train)
y_pred_SVM_classifier = SVM_classifier.predict(X_test)
SVM_classifier_Score = cross_val_score(SVM_classifier, X_train, y_train, cv=5)

print("<>================================================================================================<>")
print("<>================================= SVM_classifier RESULTS: ======================================<>")
print("<>================================================================================================<>")
# confusion matrix
# pass truth and prediction in cm function 
# confusion_matrix(truth, prediction)
cm_SVM_classifier = confusion_matrix(y_test, y_pred_SVM_classifier)
accuracy_SVM_classifier = accuracy_score(y_test, y_pred_SVM_classifier)
# cross_val_score
print("\n<>-------------- The 2 fold SVM_classifier_Score cross validation: -------<>")
print(SVM_classifier_Score)
# Getting mean
print("\n<>----------- SVM_classifier_Score Mean and Standard Deviation: ----------<>")
print(SVM_classifier_Score.mean(), SVM_classifier_Score.std())
print("\n<>----------- SVM_classifier confusion_matrix: ---------------------------<>")
print(cm_SVM_classifier)
print("\n<>----------- SVM_classifier accuracy: -----------------------------------<>")
print(accuracy_SVM_classifier)
print("\n<>----------- SVM_classifier Classification report: ----------------------<>")
print(metrics.classification_report(y_test, y_pred_SVM_classifier))
print("\n<>----------- SVM_classifier confusion_matrix plot: ----------------------<>")
# Code to plot true positives and true negatives based on confusion matrix
# Reference https://www.stackvidhya.com/plot-confusion-matrix-in-python-and-why/
# Reference https://www.ritchieng.com/machine-learning-evaluate-classification-model/
# True Negatives (TN): Model correctly predicted that HeartDisease 0
# False Negatives (FN): Model incorrectly predicted that HeartDisease 0
# True Positives (TP): Model correctly predicted that HeartDisease 1
# False Positives (FP): Model incorrectly predicted that HeartDisease 1

group_names = ['True Neg','False Pos','False Neg','True Pos']

group_counts = ["{0:0.0f}".format(value) for value in
                cm_SVM_classifier.flatten()]

group_percentages = ["{0:.2%}".format(value) for value in
                     cm_SVM_classifier.flatten()/np.sum(cm_SVM_classifier)]

labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
          zip(group_names,group_counts,group_percentages)]

labels = np.asarray(labels).reshape(2,2)

ax = sns.heatmap(cm_SVM_classifier, annot=labels, fmt='', cmap='Blues')

ax.set_title('Seaborn Confusion Matrix SVM_classifier with labels\n\n');
ax.set_xlabel('\nPredicted Values')
ax.set_ylabel('Actual Values ');

## Ticket labels - List must be in alphabetical order
ax.xaxis.set_ticklabels(['HeartDisease 0','HeartDisease 1'])
ax.yaxis.set_ticklabels(['HeartDisease 0','HeartDisease 1'])

## Display the visualization of the Confusion Matrix.
plt.show()

#Decision tree classifier
DecisionTree_classifier = DecisionTreeClassifier()
DecisionTree_classifier.fit(X_train, y_train)
y_pred_DecisionTree_classifier = DecisionTree_classifier.predict(X_test)

print("<>================================================================================================<>")
print("<>============================== DecisionTree_classifier RESULTS: ================================<>")
print("<>================================================================================================<>")
# confusion matrix
# pass truth and prediction in cm function 
# confusion_matrix(truth, prediction)
cm_DecisionTree_classifier = confusion_matrix(y_test, y_pred_DecisionTree_classifier)
accuracy_DecisionTree_classifier = accuracy_score(y_test, y_pred_DecisionTree_classifier)
# cross_val_score
DecisionTree_classifier_Score = cross_val_score(DecisionTree_classifier, X_train, y_train, cv=5)
print("\n<>----- The 5 fold DecisionTree_classifier_Score cross validation: ----------<>")
print(DecisionTree_classifier_Score)
# Getting mean
print("\n<>----- DecisionTree_classifier_Score Mean and Standard Deviation: ----------<>")
print(DecisionTree_classifier_Score.mean(), DecisionTree_classifier_Score.std())
print("\n<>----- DecisionTree_classifier confusion_matrix: ---------------------------<>")
print(cm_DecisionTree_classifier)
print("\n<>----- DecisionTree_classifier accuracy: -----------------------------------<>")
print(accuracy_DecisionTree_classifier)
print("\n<>----- DecisionTree_classifier Classification report: ----------------------<>")
print(metrics.classification_report(y_test, y_pred_DecisionTree_classifier))
print("\n<>----- DecisionTree_classifier confusion_matrix plot: ----------------------<>")
# Code to plot true positives and true negatives based on confusion matrix
# Reference https://www.stackvidhya.com/plot-confusion-matrix-in-python-and-why/
# Reference https://www.ritchieng.com/machine-learning-evaluate-classification-model/
# True Negatives (TN): Model correctly predicted that HeartDisease 0 
# False Negatives (FN): Model incorrectly predicted that HeartDisease 0 
# True Positives (TP): Model correctly predicted that HeartDisease 1 
# False Positives (FP): Model incorrectly predicted that HeartDisease 1

group_names = ['True Neg','False Pos','False Neg','True Pos']

group_counts = ["{0:0.0f}".format(value) for value in
                cm_DecisionTree_classifier.flatten()]

group_percentages = ["{0:.2%}".format(value) for value in
                     cm_DecisionTree_classifier.flatten()/np.sum(cm_DecisionTree_classifier)]

labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
          zip(group_names,group_counts,group_percentages)]

labels = np.asarray(labels).reshape(2,2)

ax = sns.heatmap(cm_DecisionTree_classifier, annot=labels, fmt='', cmap='Blues')

ax.set_title('Seaborn Confusion Matrix DecisionTree_classifier with labels\n\n');
ax.set_xlabel('\nPredicted Values')
ax.set_ylabel('Actual Values ');

## Ticket labels - List must be in alphabetical order
ax.xaxis.set_ticklabels(['HeartDisease 0','HeartDisease 1'])
ax.yaxis.set_ticklabels(['HeartDisease 0','HeartDisease 1'])

## Display the visualization of the Confusion Matrix.
plt.show()

# Tune the model Decision tree classifier using GridSearchCV to find the optimal number of trees.
# This will give us the optimal depth of the decision tree.
# Optimal depth of the decision tree:  {'max_depth': 1} with cv=5

print("<>================================================================================================<>")
print("<>==================== DecisionTree_classifier RESULTS with GridSearchCV Tuning: ===================<>")
print("<>================================================================================================<>")
#print("\n<>----------- GaussianNB_classifier.get_params-----------------------------<>")
#print(DecisionTree_classifier.get_params())
param_grid_dt = {
    'max_depth': [1, 3, 5, 7, 9, 11, 13, 15]
}
DecisionTree_classifier_grid=GridSearchCV(DecisionTree_classifier, param_grid=param_grid_dt, n_jobs=-1, cv=5, verbose=5)
DecisionTree_classifier_grid.fit(X_train, y_train)
#print("\n<>----------- DecisionTree_classifier_grid.cv_results_-----------------------<>")
#print(DecisionTree_classifier_grid.cv_results_)
#DecisionTree_classifier_grid_df = pd.DataFrame(DecisionTree_classifier_grid.cv_results_)
#print("\n<>----------- DecisionTree_classifier_grid.cv_results_ as dataframe----------<>")
#print(DecisionTree_classifier_grid_df)
print("Optimal depth of the decision tree: ",DecisionTree_classifier_grid.best_params_)

y_pred_DecisionTree_classifier_grid_optimalTreeDepth = DecisionTree_classifier_grid.predict(X_test)

print("<>================================================================================================<>")
print("<>============================== DecisionTree_classifier_grid RESULTS: ================================<>")
print("<>================================================================================================<>")
# confusion matrix
# pass truth and prediction in cm function 
# confusion_matrix(truth, prediction)
cm_DecisionTree_classifier_grid = confusion_matrix(y_test, y_pred_DecisionTree_classifier_grid_optimalTreeDepth)
accuracy_DecisionTree_classifier_grid = accuracy_score(y_test, y_pred_DecisionTree_classifier_grid_optimalTreeDepth)
# cross_val_score
DecisionTree_classifier_grid_Score = cross_val_score(DecisionTree_classifier_grid, X_train, y_train, cv=5)
print("\n<>----- The 5 fold DecisionTree_classifier_grid_Score cross validation: ----------<>")
print(DecisionTree_classifier_grid_Score)
# Getting mean
print("\n<>----- DecisionTree_classifier_grid_Score Mean and Standard Deviation: ----------<>")
print(DecisionTree_classifier_grid_Score.mean(), DecisionTree_classifier_grid_Score.std())
print("\n<>----- DecisionTree_classifier_grid confusion_matrix: ---------------------------<>")
print(cm_DecisionTree_classifier_grid)
print("\n<>----- DecisionTree_classifier_grid accuracy: -----------------------------------<>")
print(accuracy_DecisionTree_classifier_grid)
print("\n<>----- DecisionTree_classifier_grid Classification report: ----------------------<>")
print(metrics.classification_report(y_test, y_pred_DecisionTree_classifier_grid_optimalTreeDepth))
print("\n<>----- DecisionTree_classifier_grid confusion_matrix plot: ----------------------<>")
# Code to plot true positives and true negatives based on confusion matrix
# Reference https://www.stackvidhya.com/plot-confusion-matrix-in-python-and-why/
# Reference https://www.ritchieng.com/machine-learning-evaluate-classification-model/
# True Negatives (TN): Model correctly predicted that HeartDisease 0 
# False Negatives (FN): Model incorrectly predicted that HeartDisease 0 
# True Positives (TP): Model correctly predicted that HeartDisease 1 
# False Positives (FP): Model incorrectly predicted that HeartDisease 1

group_names = ['True Neg','False Pos','False Neg','True Pos']

group_counts = ["{0:0.0f}".format(value) for value in
                cm_DecisionTree_classifier_grid.flatten()]

group_percentages = ["{0:.2%}".format(value) for value in
                     cm_DecisionTree_classifier_grid.flatten()/np.sum(cm_DecisionTree_classifier_grid)]

labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
          zip(group_names,group_counts,group_percentages)]

labels = np.asarray(labels).reshape(2,2)

ax = sns.heatmap(cm_DecisionTree_classifier_grid, annot=labels, fmt='', cmap='Blues')

ax.set_title('Seaborn Confusion Matrix DecisionTree_classifier_grid with labels\n\n');
ax.set_xlabel('\nPredicted Values')
ax.set_ylabel('Actual Values ');

## Ticket labels - List must be in alphabetical order
ax.xaxis.set_ticklabels(['HeartDisease 0','HeartDisease 1'])
ax.yaxis.set_ticklabels(['HeartDisease 0','HeartDisease 1'])

## Display the visualization of the Confusion Matrix.
plt.show()

# XGboost classifier
xgBoost_classifier = xgb.XGBClassifier()
xgBoost_classifier.fit(X_train, y_train)
y_pred_xgBoost_classifier = xgBoost_classifier.predict(X_test)

print("<>================================================================================================<>")
print("<>============================== xgBoost_classifier RESULTS: ================================<>")
print("<>================================================================================================<>")
# confusion matrix
# pass truth and prediction in cm function 
# confusion_matrix(truth, prediction)
cm_xgBoost_classifier = confusion_matrix(y_test, y_pred_xgBoost_classifier)
accuracy_xgBoost_classifier = accuracy_score(y_test, y_pred_xgBoost_classifier)
# cross_val_score
xgBoost_classifier_Score = cross_val_score(xgBoost_classifier, X_train, y_train, cv=5)
print("\n<>----- The 5 fold xgBoost_classifier_Score cross validation: ----------<>")
print(xgBoost_classifier_Score)
# Getting mean
print("\n<>----- xgBoost_classifier_Score Mean and Standard Deviation: ----------<>")
print(xgBoost_classifier_Score.mean(), xgBoost_classifier_Score.std())
print("\n<>----- xgBoost_classifier confusion_matrix: ---------------------------<>")
print(cm_xgBoost_classifier)
print("\n<>----- xgBoost_classifier accuracy: -----------------------------------<>")
print(accuracy_xgBoost_classifier)
print("\n<>----- xgBoost_classifier Classification report: ----------------------<>")
print(metrics.classification_report(y_test, y_pred_xgBoost_classifier))
print("\n<>----- xgBoost_classifier confusion_matrix plot: ----------------------<>")
# Code to plot true positives and true negatives based on confusion matrix
# Reference https://www.stackvidhya.com/plot-confusion-matrix-in-python-and-why/
# Reference https://www.ritchieng.com/machine-learning-evaluate-classification-model/
# True Negatives (TN): Model correctly predicted that HeartDisease 0 
# False Negatives (FN): Model incorrectly predicted that HeartDisease 0 
# True Positives (TP): Model correctly predicted that HeartDisease 1 
# False Positives (FP): Model incorrectly predicted that HeartDisease 1

group_names = ['True Neg','False Pos','False Neg','True Pos']

group_counts = ["{0:0.0f}".format(value) for value in
                cm_xgBoost_classifier.flatten()]

group_percentages = ["{0:.2%}".format(value) for value in
                     cm_xgBoost_classifier.flatten()/np.sum(cm_xgBoost_classifier)]

labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
          zip(group_names,group_counts,group_percentages)]

labels = np.asarray(labels).reshape(2,2)

ax = sns.heatmap(cm_xgBoost_classifier, annot=labels, fmt='', cmap='Blues')

ax.set_title('Seaborn Confusion Matrix xgBoost_classifier with labels\n\n');
ax.set_xlabel('\nPredicted Values')
ax.set_ylabel('Actual Values ');

## Ticket labels - List must be in alphabetical order
ax.xaxis.set_ticklabels(['HeartDisease 0','HeartDisease 1'])
ax.yaxis.set_ticklabels(['HeartDisease 0','HeartDisease 1'])

## Display the visualization of the Confusion Matrix.
plt.show()

gradBoost_classifier = GradientBoostingClassifier()
gradBoost_classifier.fit(X_train, y_train)
y_pred_gradBoost_classifier = gradBoost_classifier.predict(X_test)

print("<>================================================================================================<>")
print("<>============================== gradBoost_classifier RESULTS: ================================<>")
print("<>================================================================================================<>")
# confusion matrix
# pass truth and prediction in cm function 
# confusion_matrix(truth, prediction)
cm_gradBoost_classifier = confusion_matrix(y_test, y_pred_gradBoost_classifier)
accuracy_gradBoost_classifier = accuracy_score(y_test, y_pred_gradBoost_classifier)
# cross_val_score
gradBoost_classifier_Score = cross_val_score(gradBoost_classifier, X_train, y_train, cv=5)
print("\n<>----- The 5 fold gradBoost_classifier_Score cross validation: ----------<>")
print(gradBoost_classifier_Score)
# Getting mean
print("\n<>----- gradBoost_classifier_Score Mean and Standard Deviation: ----------<>")
print(gradBoost_classifier_Score.mean(), gradBoost_classifier_Score.std())
print("\n<>----- gradBoost_classifier confusion_matrix: ---------------------------<>")
print(cm_gradBoost_classifier)
print("\n<>----- gradBoost_classifier accuracy: -----------------------------------<>")
print(accuracy_gradBoost_classifier)
print("\n<>----- gradBoost_classifier Classification report: ----------------------<>")
print(metrics.classification_report(y_test, y_pred_gradBoost_classifier))
print("\n<>----- gradBoost_classifier confusion_matrix plot: ----------------------<>")
# Code to plot true positives and true negatives based on confusion matrix
# Reference https://www.stackvidhya.com/plot-confusion-matrix-in-python-and-why/
# Reference https://www.ritchieng.com/machine-learning-evaluate-classification-model/
# True Negatives (TN): Model correctly predicted that HeartDisease 0 
# False Negatives (FN): Model incorrectly predicted that HeartDisease 0 
# True Positives (TP): Model correctly predicted that HeartDisease 1 
# False Positives (FP): Model incorrectly predicted that HeartDisease 1

group_names = ['True Neg','False Pos','False Neg','True Pos']

group_counts = ["{0:0.0f}".format(value) for value in
                cm_gradBoost_classifier.flatten()]

group_percentages = ["{0:.2%}".format(value) for value in
                     cm_gradBoost_classifier.flatten()/np.sum(cm_gradBoost_classifier)]

labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
          zip(group_names,group_counts,group_percentages)]

labels = np.asarray(labels).reshape(2,2)

ax = sns.heatmap(cm_gradBoost_classifier, annot=labels, fmt='', cmap='Blues')

ax.set_title('Seaborn Confusion Matrix gradBoost_classifier with labels\n\n');
ax.set_xlabel('\nPredicted Values')
ax.set_ylabel('Actual Values ');

## Ticket labels - List must be in alphabetical order
ax.xaxis.set_ticklabels(['HeartDisease 0','HeartDisease 1'])
ax.yaxis.set_ticklabels(['HeartDisease 0','HeartDisease 1'])

## Display the visualization of the Confusion Matrix.
plt.show()

"""**Combining classifiers KNN (k=5), SVM (kernel = rbf), DT with optimal tree number, and xgboost classifiers into a single soft/hard voting classifier.**

These classiifiers were choses in voting classifier as it was the requirement of the assignment: 

Use a majority voting approach to predict class label using KNN (k=5), SVM (kernel = rbf), DT (ensure you find optimal tree), and XGboost classifiers Note: In majority voting, the predicted class label for a particular sample is the class label that represents the majority of the class labels predicted by each individual classifier.

"""

# Combining all above classifiers (KNN (k=5), SVM (kernel = rbf), DT with optimal tree number, and xgboost classifiers) into a single soft voting classifier.
votesoft_classifier = VotingClassifier(
    estimators=[('KNN', KNN_k5_classifier),
                ('SVM', SVM_classifier), 
                ('DT', DecisionTree_classifier_grid), 
                ('XGboost', xgBoost_classifier)],
    voting='soft')
votesoft_classifier.fit(X_train, y_train)
y_pred_votesoft_classifier = votesoft_classifier.predict(X_test)

print("<>================================================================================================<>")
print("<>============================== votesoft_classifier RESULTS: ================================<>")
print("<>================================================================================================<>")
# confusion matrix
# pass truth and prediction in cm function 
# confusion_matrix(truth, prediction)
cm_votesoft_classifier = confusion_matrix(y_test, y_pred_votesoft_classifier)
accuracy_votesoft_classifier = accuracy_score(y_test, y_pred_votesoft_classifier)
# cross_val_score
votesoft_classifier_Score = cross_val_score(votesoft_classifier, X_train, y_train, cv=5)
print("\n<>----- The 5 fold votesoft_classifier_Score cross validation: ----------<>")
print(votesoft_classifier_Score)
# Getting mean
print("\n<>----- votesoft_classifier_Score Mean and Standard Deviation: ----------<>")
print(votesoft_classifier_Score.mean(), votesoft_classifier_Score.std())
print("\n<>----- votesoft_classifier confusion_matrix: ---------------------------<>")
print(cm_votesoft_classifier)
print("\n<>----- votesoft_classifier accuracy: -----------------------------------<>")
print(accuracy_votesoft_classifier)
print("\n<>----- votesoft_classifier Classification report: ----------------------<>")
print(metrics.classification_report(y_test, y_pred_votesoft_classifier))
print("\n<>----- votesoft_classifier confusion_matrix plot: ----------------------<>")
# Code to plot true positives and true negatives based on confusion matrix
# Reference https://www.stackvidhya.com/plot-confusion-matrix-in-python-and-why/
# Reference https://www.ritchieng.com/machine-learning-evaluate-classification-model/
# True Negatives (TN): Model correctly predicted that HeartDisease 0 
# False Negatives (FN): Model incorrectly predicted that HeartDisease 0 
# True Positives (TP): Model correctly predicted that HeartDisease 1 
# False Positives (FP): Model incorrectly predicted that HeartDisease 1

group_names = ['True Neg','False Pos','False Neg','True Pos']

group_counts = ["{0:0.0f}".format(value) for value in
                cm_votesoft_classifier.flatten()]

group_percentages = ["{0:.2%}".format(value) for value in
                     cm_votesoft_classifier.flatten()/np.sum(cm_votesoft_classifier)]

labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
          zip(group_names,group_counts,group_percentages)]

labels = np.asarray(labels).reshape(2,2)

ax = sns.heatmap(cm_votesoft_classifier, annot=labels, fmt='', cmap='Blues')

ax.set_title('Seaborn Confusion Matrix votesoft_classifier with labels\n\n');
ax.set_xlabel('\nPredicted Values')
ax.set_ylabel('Actual Values ');

## Ticket labels - List must be in alphabetical order
ax.xaxis.set_ticklabels(['HeartDisease 0','HeartDisease 1'])
ax.yaxis.set_ticklabels(['HeartDisease 0','HeartDisease 1'])

## Display the visualization of the Confusion Matrix.
plt.show()

votehard_classifier = VotingClassifier(
    estimators=[('KNN', KNN_k5_classifier),
                ('SVM', SVM_classifier), 
                ('DT', DecisionTree_classifier_grid), 
                ('XGboost', xgBoost_classifier)],
    voting='hard')
votehard_classifier.fit(X_train, y_train)
y_pred_votehard_classifier = votehard_classifier.predict(X_test)

print("<>================================================================================================<>")
print("<>============================== votehard_classifier RESULTS: ================================<>")
print("<>================================================================================================<>")
# confusion matrix
# pass truth and prediction in cm function 
# confusion_matrix(truth, prediction)
cm_votehard_classifier = confusion_matrix(y_test, y_pred_votehard_classifier)
accuracy_votehard_classifier = accuracy_score(y_test, y_pred_votehard_classifier)
# cross_val_score
votehard_classifier_Score = cross_val_score(votehard_classifier, X_train, y_train, cv=5)
print("\n<>----- The 5 fold votehard_classifier_Score cross validation: ----------<>")
print(votehard_classifier_Score)
# Getting mean
print("\n<>----- votehard_classifier_Score Mean and Standard Deviation: ----------<>")
print(votehard_classifier_Score.mean(), votehard_classifier_Score.std())
print("\n<>----- votehard_classifier confusion_matrix: ---------------------------<>")
print(cm_votehard_classifier)
print("\n<>----- votehard_classifier accuracy: -----------------------------------<>")
print(accuracy_votehard_classifier)
print("\n<>----- votehard_classifier Classification report: ----------------------<>")
print(metrics.classification_report(y_test, y_pred_votehard_classifier))
print("\n<>----- votehard_classifier confusion_matrix plot: ----------------------<>")
# Code to plot true positives and true negatives based on confusion matrix
# Reference https://www.stackvidhya.com/plot-confusion-matrix-in-python-and-why/
# Reference https://www.ritchieng.com/machine-learning-evaluate-classification-model/
# True Negatives (TN): Model correctly predicted that HeartDisease 0 
# False Negatives (FN): Model incorrectly predicted that HeartDisease 0 
# True Positives (TP): Model correctly predicted that HeartDisease 1 
# False Positives (FP): Model incorrectly predicted that HeartDisease 1

group_names = ['True Neg','False Pos','False Neg','True Pos']

group_counts = ["{0:0.0f}".format(value) for value in
                cm_votehard_classifier.flatten()]

group_percentages = ["{0:.2%}".format(value) for value in
                     cm_votehard_classifier.flatten()/np.sum(cm_votehard_classifier)]

labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
          zip(group_names,group_counts,group_percentages)]

labels = np.asarray(labels).reshape(2,2)

ax = sns.heatmap(cm_votehard_classifier, annot=labels, fmt='', cmap='Blues')

ax.set_title('Seaborn Confusion Matrix votehard_classifier with labels\n\n');
ax.set_xlabel('\nPredicted Values')
ax.set_ylabel('Actual Values ');

## Ticket labels - List must be in alphabetical order
ax.xaxis.set_ticklabels(['HeartDisease 0','HeartDisease 1'])
ax.yaxis.set_ticklabels(['HeartDisease 0','HeartDisease 1'])

## Display the visualization of the Confusion Matrix.
plt.show()

"""# Model Development II

Deep learning with Keras

3 models are explored with different activation functions in the first hidden layer of the model:

(1)Keras_model1_clf with activation='tanh'

(2)Keras_model2_clf with activation='relu'

(3)Keras_model3_clf with activation='sigmoid'

"""

# Keras 
# Reference: https://blog.pythian.com/heart-disease-prediction-using-keras-deep-learning/
from keras.models import Sequential
from keras.layers import Dense

# In the first line, we set the model as sequential. 
# Then, we add the three fully connected dense layers: two hidden and one output. 
# These are defined using the dense class. The first level has a dimension of 
# 11 which corresponds to 11 column attributes in X.

# Use tanh to set the activation function
# The second layer has 20 neurons and the tanh activation function
# The output layer has a single neuron (output) and the sigmoid activation 
# function suited for binary classification problems

Keras_model1_clf = Sequential()
Keras_model1_clf.add(Dense(30, input_dim=6, activation='tanh'))
Keras_model1_clf.add(Dense(20, activation='tanh'))
Keras_model1_clf.add(Dense(1, activation='sigmoid'))

# Use relu to set the activation function
# The second layer has 20 neurons and the tanh activation function
# The output layer has a single neuron (output) and the sigmoid activation 
# function suited for binary classification problems
Keras_model2_clf = Sequential()
Keras_model2_clf.add(Dense(30, input_dim=6, activation='relu'))
Keras_model2_clf.add(Dense(20, activation='relu'))
Keras_model2_clf.add(Dense(1, activation='sigmoid'))

# Use sigmoid to set the activation function
# The second layer has 20 neurons and the tanh activation function
# The output layer has a single neuron (output) and the sigmoid activation 
# function suited for binary classification problems
Keras_model3_clf = Sequential()
Keras_model3_clf.add(Dense(30, input_dim=6, activation='sigmoid'))
Keras_model3_clf.add(Dense(20, activation='sigmoid'))
Keras_model3_clf.add(Dense(1, activation='sigmoid'))


# The compile function has three arguments
# adam optimizer: An algorithm for first-order gradient-based optimization.
# binary_crossentropy loss function: logarithmic loss, which for a binary 
# classification problem is defined in Keras as binary_crossentropy.
# accuracy metric: to evaluate the performance of your model during training 
# and testing
# set the epochs=100 and let the model train

Keras_model1_clf.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])
Keras_model1_clf.fit(X_train, y_train, epochs=100, batch_size=10)

Keras_model2_clf.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])
Keras_model2_clf.fit(X_train, y_train, epochs=100, batch_size=10)

Keras_model3_clf.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])
Keras_model3_clf.fit(X_train, y_train, epochs=100, batch_size=10)

"""Keras_model1_clf with activation='tanh'"""

y_pred_Keras_model1_clf = Keras_model1_clf.predict(X_test)

print("<>================================================================================================<>")
print("<>============================== Keras_model1_clf RESULTS: ==================================<>")
print("<>================================================================================================<>")
# confusion matrix
# pass truth and prediction in cm function 
# confusion_matrix(truth, prediction)
cm_Keras_model1_clf = confusion_matrix(y_test, np.round(y_pred_Keras_model1_clf))
accuracy_Keras_model1_clf = accuracy_score(y_test, np.round(y_pred_Keras_model1_clf))


print ("\n<>-----Keras_model1_clf Summary: ---------------------------<>")
print (Keras_model1_clf.summary())

print ("\n<>-----Keras_model1_clf score: ---------------------------<>")
score_Keras_model1_clf = Keras_model1_clf.evaluate(X_test, y_test, verbose=0)
print('Keras_model1_clf score = ',score_Keras_model1_clf[1])

print("\n<>-----Keras_model1_clf confusion_matrix: ---------------------------<>")
print(cm_Keras_model1_clf)
print("\n<>-----Keras_model1_clf accuracy: -----------------------------------<>")
print(accuracy_Keras_model1_clf)
print("\n<>-----Keras_model1_clf Classification report: ----------------------<>")
print(metrics.classification_report(y_test, np.round(y_pred_Keras_model1_clf)))
print("\n<>-----Keras_model1_clf confusion_matrix plot: ----------------------<>")
# Code to plot true positives and true negatives based on confusion matrix
# Reference https://www.stackvidhya.com/plot-confusion-matrix-in-python-and-why/
# Reference https://www.ritchieng.com/machine-learning-evaluate-classification-model/
# True Negatives (TN): Model correctly predicted that HeartDisease 0
# False Negatives (FN): Model incorrectly predicted that HeartDisease 0 
# True Positives (TP): Model correctly predicted that HeartDisease 1
# False Positives (FP): Model incorrectly predicted that HeartDisease 1

group_names = ['True Neg','False Pos','False Neg','True Pos']

group_counts = ["{0:0.0f}".format(value) for value in
                cm_Keras_model1_clf.flatten()]

group_percentages = ["{0:.2%}".format(value) for value in
                     cm_Keras_model1_clf.flatten()/np.sum(cm_Keras_model1_clf)]

labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
          zip(group_names,group_counts,group_percentages)]

labels = np.asarray(labels).reshape(2,2)

ax = sns.heatmap(cm_Keras_model1_clf, annot=labels, fmt='', cmap='Blues')

ax.set_title('Seaborn Confusion Matrix Keras_model1_clf with labels\n\n');
ax.set_xlabel('\nPredicted Values')
ax.set_ylabel('Actual Values ');

## Ticket labels - List must be in alphabetical order
ax.xaxis.set_ticklabels(['HeartDisease 0','HeartDisease 1'])
ax.yaxis.set_ticklabels(['HeartDisease 0','HeartDisease 1'])

## Display the visualization of the Confusion Matrix.
plt.show()

"""Keras_model2_clf with activation='relu'"""

y_pred_Keras_model2_clf = Keras_model2_clf.predict(X_test)

print("<>================================================================================================<>")
print("<>============================== Keras_model2_clf RESULTS: ==================================<>")
print("<>================================================================================================<>")
# confusion matrix
# pass truth and prediction in cm function 
# confusion_matrix(truth, prediction)
cm_Keras_model2_clf = confusion_matrix(y_test, np.round(y_pred_Keras_model2_clf))
accuracy_Keras_model2_clf = accuracy_score(y_test, np.round(y_pred_Keras_model2_clf))


print ("\n<>-----Keras_model2_clf Summary: ---------------------------<>")
print (Keras_model2_clf.summary())

print ("\n<>-----Keras_model2_clf score: ---------------------------<>")
score_Keras_model2_clf = Keras_model2_clf.evaluate(X_test, y_test, verbose=0)
print('Keras_model2_clf score = ',score_Keras_model2_clf[1])

print("\n<>-----Keras_model2_clf confusion_matrix: ---------------------------<>")
print(cm_Keras_model2_clf)
print("\n<>-----Keras_model2_clf accuracy: -----------------------------------<>")
print(accuracy_Keras_model2_clf)
print("\n<>-----Keras_model2_clf Classification report: ----------------------<>")
print(metrics.classification_report(y_test, np.round(y_pred_Keras_model2_clf)))
print("\n<>-----Keras_model2_clf confusion_matrix plot: ----------------------<>")
# Code to plot true positives and true negatives based on confusion matrix
# Reference https://www.stackvidhya.com/plot-confusion-matrix-in-python-and-why/
# Reference https://www.ritchieng.com/machine-learning-evaluate-classification-model/
# True Negatives (TN): Model correctly predicted that HeartDisease 0
# False Negatives (FN): Model incorrectly predicted that HeartDisease 0 
# True Positives (TP): Model correctly predicted that HeartDisease 1
# False Positives (FP): Model incorrectly predicted that HeartDisease 1

group_names = ['True Neg','False Pos','False Neg','True Pos']

group_counts = ["{0:0.0f}".format(value) for value in
                cm_Keras_model2_clf.flatten()]

group_percentages = ["{0:.2%}".format(value) for value in
                     cm_Keras_model2_clf.flatten()/np.sum(cm_Keras_model2_clf)]

labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
          zip(group_names,group_counts,group_percentages)]

labels = np.asarray(labels).reshape(2,2)

ax = sns.heatmap(cm_Keras_model2_clf, annot=labels, fmt='', cmap='Blues')

ax.set_title('Seaborn Confusion Matrix Keras_model2_clf with labels\n\n');
ax.set_xlabel('\nPredicted Values')
ax.set_ylabel('Actual Values ');

## Ticket labels - List must be in alphabetical order
ax.xaxis.set_ticklabels(['HeartDisease 0','HeartDisease 1'])
ax.yaxis.set_ticklabels(['HeartDisease 0','HeartDisease 1'])

## Display the visualization of the Confusion Matrix.
plt.show()

y_pred_Keras_model3_clf = Keras_model3_clf.predict(X_test)

print("<>================================================================================================<>")
print("<>============================== Keras_model3_clf RESULTS: ==================================<>")
print("<>================================================================================================<>")
# confusion matrix
# pass truth and prediction in cm function 
# confusion_matrix(truth, prediction)
cm_Keras_model3_clf = confusion_matrix(y_test, np.round(y_pred_Keras_model3_clf))
accuracy_Keras_model3_clf = accuracy_score(y_test, np.round(y_pred_Keras_model3_clf))


print ("\n<>-----Keras_model3_clf Summary: ---------------------------<>")
print (Keras_model3_clf.summary())

print ("\n<>-----Keras_model3_clf score: ---------------------------<>")
score_Keras_model3_clf = Keras_model3_clf.evaluate(X_test, y_test, verbose=0)
print('Keras_model3_clf score = ',score_Keras_model3_clf[1])

print("\n<>-----Keras_model3_clf confusion_matrix: ---------------------------<>")
print(cm_Keras_model3_clf)
print("\n<>-----Keras_model3_clf accuracy: -----------------------------------<>")
print(accuracy_Keras_model3_clf)
print("\n<>-----Keras_model3_clf Classification report: ----------------------<>")
print(metrics.classification_report(y_test, np.round(y_pred_Keras_model3_clf)))
print("\n<>-----Keras_model3_clf confusion_matrix plot: ----------------------<>")
# Code to plot true positives and true negatives based on confusion matrix
# Reference https://www.stackvidhya.com/plot-confusion-matrix-in-python-and-why/
# Reference https://www.ritchieng.com/machine-learning-evaluate-classification-model/
# True Negatives (TN): Model correctly predicted that HeartDisease 0
# False Negatives (FN): Model incorrectly predicted that HeartDisease 0 
# True Positives (TP): Model correctly predicted that HeartDisease 1
# False Positives (FP): Model incorrectly predicted that HeartDisease 1

group_names = ['True Neg','False Pos','False Neg','True Pos']

group_counts = ["{0:0.0f}".format(value) for value in
                cm_Keras_model3_clf.flatten()]

group_percentages = ["{0:.2%}".format(value) for value in
                     cm_Keras_model3_clf.flatten()/np.sum(cm_Keras_model3_clf)]

labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
          zip(group_names,group_counts,group_percentages)]

labels = np.asarray(labels).reshape(2,2)

ax = sns.heatmap(cm_Keras_model3_clf, annot=labels, fmt='', cmap='Blues')

ax.set_title('Seaborn Confusion Matrix Keras_model3_clf with labels\n\n');
ax.set_xlabel('\nPredicted Values')
ax.set_ylabel('Actual Values ');

## Ticket labels - List must be in alphabetical order
ax.xaxis.set_ticklabels(['HeartDisease 0','HeartDisease 1'])
ax.yaxis.set_ticklabels(['HeartDisease 0','HeartDisease 1'])

## Display the visualization of the Confusion Matrix.
plt.show()

total1_KNN_k5=sum(sum(cm_KNN_k5_classifier))
Accuracy_KNN_k5 = (cm_KNN_k5_classifier[0,0]+cm_KNN_k5_classifier[1,1])/total1_KNN_k5
Precision_KNN_k5 = cm_KNN_k5_classifier[1,1]/(cm_KNN_k5_classifier[1,1]+cm_KNN_k5_classifier[0,1])
Sensitivity_recall_KNN_k5 = cm_KNN_k5_classifier[1,1]/(cm_KNN_k5_classifier[1,0]+cm_KNN_k5_classifier[1,1])
F1_KNN_k5 = 2*(Sensitivity_recall_KNN_k5 * Precision_KNN_k5) / (Sensitivity_recall_KNN_k5 + Precision_KNN_k5)
Specificity_KNN_k5 = cm_KNN_k5_classifier[0,0]/(cm_KNN_k5_classifier[0,0]+cm_KNN_k5_classifier[0,1])

total1_SVM_rbf=sum(sum(cm_SVM_classifier))
Accuracy_SVM_rbf = (cm_SVM_classifier[0,0]+cm_SVM_classifier[1,1])/total1_SVM_rbf
Precision_SVM_rbf = cm_SVM_classifier[1,1]/(cm_SVM_classifier[1,1]+cm_SVM_classifier[0,1])
Sensitivity_recall_SVM_rbf = cm_SVM_classifier[1,1]/(cm_SVM_classifier[1,0]+cm_SVM_classifier[1,1])
F1_SVM_rbf = 2*(Sensitivity_recall_SVM_rbf * Precision_SVM_rbf) / (Sensitivity_recall_SVM_rbf + Precision_SVM_rbf)
Specificity_SVM_rbf = cm_SVM_classifier[0,0]/(cm_SVM_classifier[0,0]+cm_SVM_classifier[0,1])

total1_DT=sum(sum(cm_DecisionTree_classifier))
Accuracy_DT = (cm_DecisionTree_classifier[0,0]+cm_DecisionTree_classifier[1,1])/total1_DT
Precision_DT = cm_DecisionTree_classifier[1,1]/(cm_DecisionTree_classifier[1,1]+cm_DecisionTree_classifier[0,1])
Sensitivity_recall_DT = cm_DecisionTree_classifier[1,1]/(cm_DecisionTree_classifier[1,0]+cm_DecisionTree_classifier[1,1])
F1_DT = 2*(Sensitivity_recall_DT * Precision_DT) / (Sensitivity_recall_DT + Precision_DT)
Specificity_DT = cm_DecisionTree_classifier[0,0]/(cm_DecisionTree_classifier[0,0]+cm_DecisionTree_classifier[0,1])

total1_DT_optimalDepth=sum(sum(cm_DecisionTree_classifier_grid))
Accuracy_DT_optimalDepth = (cm_DecisionTree_classifier_grid[0,0]+cm_DecisionTree_classifier_grid[1,1])/total1_DT_optimalDepth
Precision_DT_optimalDepth = cm_DecisionTree_classifier_grid[1,1]/(cm_DecisionTree_classifier_grid[1,1]+cm_DecisionTree_classifier_grid[0,1])
Sensitivity_recall_DT_optimalDepth = cm_DecisionTree_classifier_grid[1,1]/(cm_DecisionTree_classifier_grid[1,0]+cm_DecisionTree_classifier_grid[1,1])
F1_DT_optimalDepth = 2*(Sensitivity_recall_DT_optimalDepth * Precision_DT_optimalDepth) / (Sensitivity_recall_DT_optimalDepth + Precision_DT_optimalDepth)
Specificity_DT_optimalDepth = cm_DecisionTree_classifier_grid[0,0]/(cm_DecisionTree_classifier_grid[0,0]+cm_DecisionTree_classifier_grid[0,1])

total1_xgBoost=sum(sum(cm_xgBoost_classifier))
Accuracy_xgBoost = (cm_xgBoost_classifier[0,0]+cm_xgBoost_classifier[1,1])/total1_xgBoost
Precision_xgBoost = cm_xgBoost_classifier[1,1]/(cm_xgBoost_classifier[1,1]+cm_xgBoost_classifier[0,1])
Sensitivity_recall_xgBoost = cm_xgBoost_classifier[1,1]/(cm_xgBoost_classifier[1,0]+cm_xgBoost_classifier[1,1])
F1_xgBoost = 2*(Sensitivity_recall_xgBoost * Precision_xgBoost) / (Sensitivity_recall_xgBoost + Precision_xgBoost)
Specificity_xgBoost = cm_xgBoost_classifier[0,0]/(cm_xgBoost_classifier[0,0]+cm_xgBoost_classifier[0,1])

total1_gradBoost=sum(sum(cm_gradBoost_classifier))
Accuracy_gradBoost = (cm_gradBoost_classifier[0,0]+cm_gradBoost_classifier[1,1])/total1_gradBoost
Precision_gradBoost = cm_gradBoost_classifier[1,1]/(cm_gradBoost_classifier[1,1]+cm_gradBoost_classifier[0,1])
Sensitivity_recall_gradBoost = cm_gradBoost_classifier[1,1]/(cm_gradBoost_classifier[1,0]+cm_gradBoost_classifier[1,1])
F1_gradBoost = 2*(Sensitivity_recall_gradBoost * Precision_gradBoost) / (Sensitivity_recall_gradBoost + Precision_gradBoost)
Specificity_gradBoost = cm_gradBoost_classifier[0,0]/(cm_gradBoost_classifier[0,0]+cm_gradBoost_classifier[0,1])

total1_voteSoft=sum(sum(cm_votesoft_classifier))
Accuracy_voteSoft = (cm_votesoft_classifier[0,0]+cm_votesoft_classifier[1,1])/total1_voteSoft
Precision_voteSoft = cm_votesoft_classifier[1,1]/(cm_votesoft_classifier[1,1]+cm_votesoft_classifier[0,1])
Sensitivity_recall_voteSoft = cm_votesoft_classifier[1,1]/(cm_votesoft_classifier[1,0]+cm_votesoft_classifier[1,1])
F1_voteSoft = 2*(Sensitivity_recall_voteSoft * Precision_voteSoft) / (Sensitivity_recall_voteSoft + Precision_voteSoft)
Specificity_voteSoft = cm_votesoft_classifier[0,0]/(cm_votesoft_classifier[0,0]+cm_votesoft_classifier[0,1])

total1_voteHard=sum(sum(cm_votehard_classifier))
Accuracy_voteHard = (cm_votehard_classifier[0,0]+cm_votehard_classifier[1,1])/total1_voteHard
Precision_voteHard = cm_votehard_classifier[1,1]/(cm_votehard_classifier[1,1]+cm_votehard_classifier[0,1])
Sensitivity_recall_voteHard = cm_votehard_classifier[1,1]/(cm_votehard_classifier[1,0]+cm_votehard_classifier[1,1])
F1_voteHard = 2*(Sensitivity_recall_voteHard * Precision_voteHard) / (Sensitivity_recall_voteHard + Precision_voteHard)
Specificity_voteHard = cm_votehard_classifier[0,0]/(cm_votehard_classifier[0,0]+cm_votehard_classifier[0,1])

total1_keras_act_tanh=sum(sum(cm_Keras_model1_clf))
Accuracy_keras_act_tanh = (cm_Keras_model1_clf[0,0]+cm_Keras_model1_clf[1,1])/total1_keras_act_tanh
Precision_keras_act_tanh = cm_Keras_model1_clf[1,1]/(cm_Keras_model1_clf[1,1]+cm_Keras_model1_clf[0,1])
Sensitivity_recall_keras_act_tanh = cm_Keras_model1_clf[1,1]/(cm_Keras_model1_clf[1,0]+cm_Keras_model1_clf[1,1])
F1_keras_act_tanh = 2*(Sensitivity_recall_keras_act_tanh * Precision_keras_act_tanh) / (Sensitivity_recall_keras_act_tanh + Precision_keras_act_tanh)
Specificity_keras_act_tanh = cm_Keras_model1_clf[0,0]/(cm_Keras_model1_clf[0,0]+cm_Keras_model1_clf[0,1])

total1_keras_act_relu=sum(sum(cm_Keras_model2_clf))
Accuracy_keras_act_relu = (cm_Keras_model2_clf[0,0]+cm_Keras_model2_clf[1,1])/total1_keras_act_relu
Precision_keras_act_relu = cm_Keras_model2_clf[1,1]/(cm_Keras_model2_clf[1,1]+cm_Keras_model2_clf[0,1])
Sensitivity_recall_keras_act_relu = cm_Keras_model2_clf[1,1]/(cm_Keras_model2_clf[1,0]+cm_Keras_model2_clf[1,1])
F1_keras_act_relu = 2*(Sensitivity_recall_keras_act_relu * Precision_keras_act_relu) / (Sensitivity_recall_keras_act_relu + Precision_keras_act_relu)
Specificity_keras_act_relu = cm_Keras_model2_clf[0,0]/(cm_Keras_model2_clf[0,0]+cm_Keras_model2_clf[0,1])

total1_keras_act_sigm=sum(sum(cm_Keras_model3_clf))
Accuracy_keras_act_sigm = (cm_Keras_model3_clf[0,0]+cm_Keras_model3_clf[1,1])/total1_keras_act_sigm
Precision_keras_act_sigm = cm_Keras_model3_clf[1,1]/(cm_Keras_model3_clf[1,1]+cm_Keras_model3_clf[0,1])
Sensitivity_recall_keras_act_sigm = cm_Keras_model3_clf[1,1]/(cm_Keras_model3_clf[1,0]+cm_Keras_model3_clf[1,1])
F1_keras_act_sigm = 2*(Sensitivity_recall_keras_act_sigm * Precision_keras_act_sigm) / (Sensitivity_recall_keras_act_sigm + Precision_keras_act_sigm)
Specificity_keras_act_sigm = cm_Keras_model3_clf[0,0]/(cm_Keras_model3_clf[0,0]+cm_Keras_model3_clf[0,1])

Accuracy = [Accuracy_KNN_k5,Accuracy_SVM_rbf,Accuracy_DT,Accuracy_DT_optimalDepth,Accuracy_xgBoost,Accuracy_gradBoost,Accuracy_voteSoft,Accuracy_voteHard,Accuracy_keras_act_tanh,Accuracy_keras_act_relu,Accuracy_keras_act_sigm]
Precision = [Precision_KNN_k5,Precision_SVM_rbf,Precision_DT,Precision_DT_optimalDepth,Precision_xgBoost,Precision_gradBoost,Precision_voteSoft,Precision_voteHard,Precision_keras_act_tanh,Precision_keras_act_relu,Precision_keras_act_sigm]
Sensitivity_recall = [Sensitivity_recall_KNN_k5,Sensitivity_recall_SVM_rbf,Sensitivity_recall_DT,Sensitivity_recall_DT_optimalDepth,Sensitivity_recall_xgBoost,Sensitivity_recall_gradBoost,Sensitivity_recall_voteSoft,Sensitivity_recall_voteHard,Sensitivity_recall_keras_act_tanh,Sensitivity_recall_keras_act_relu,Sensitivity_recall_keras_act_sigm]
F1 = [F1_KNN_k5,F1_SVM_rbf,F1_DT,F1_DT_optimalDepth,F1_xgBoost,F1_gradBoost,F1_voteSoft,F1_voteHard,F1_keras_act_tanh,F1_keras_act_relu,F1_keras_act_sigm]
Specificity = [Specificity_KNN_k5,Specificity_SVM_rbf,Specificity_DT,Specificity_DT_optimalDepth,Specificity_xgBoost,Specificity_gradBoost,Specificity_voteSoft,Specificity_voteHard,Specificity_keras_act_tanh,Specificity_keras_act_relu,Specificity_keras_act_sigm]

# Plot all measures for all classifiers
#scores = [accuracy_KNN_k5_classifier,accuracy_SVM_classifier,accuracy_DecisionTree_classifier,accuracy_DecisionTree_classifier_grid,accuracy_xgBoost_classifier,accuracy_gradBoost_classifier,accuracy_votesoft_classifier,accuracy_votehard_classifier,accuracy_Keras_model1_clf,accuracy_Keras_model2_clf,accuracy_Keras_model3_clf]
algorithms = ["KNN_k5","SVM_rbf", "DT", "DT_OptDepth", "xgBoost", "gradBoost", "voteSoft", "voteHard", "keras_tan", "keras_rel", "keras_sig" ] 

plt.rcParams['figure.figsize'] = (27, 27)

plt.subplot(3, 2, 1)
sns.set(rc={'figure.figsize':(30,30)})
plt.xlabel("Algorithms Bar graph score for Accuracy")
plt.ylabel("Accuracy = (TP+TN)/(TP+FP+FN+TN)")
sns.barplot(algorithms,Accuracy)

plt.subplot(3, 2, 2)
sns.set(rc={'figure.figsize':(30,30)})
plt.xlabel("Algorithms Bar graph score for Precision")
plt.ylabel("Precision = TP/(TP+FP)")
sns.barplot(algorithms,Precision)

plt.subplot(3, 2, 3)
sns.set(rc={'figure.figsize':(30,30)})
plt.xlabel("Algorithms Bar graph score for Sensitivity_recall")
plt.ylabel("Recall = TP/(TP+FN) (aka Sensitivity)")
sns.barplot(algorithms,Sensitivity_recall)

plt.subplot(3, 2, 4)
sns.set(rc={'figure.figsize':(30,30)})
plt.xlabel("Algorithms Bar graph score for F1")
plt.ylabel("F1 Score = 2*(Recall * Precision) / (Recall + Precision)")
sns.barplot(algorithms,F1)

plt.subplot(3, 2, 5)
sns.set(rc={'figure.figsize':(30,30)})
plt.xlabel("Algorithms Bar graph score for Specificity")
plt.ylabel("Specificity = TN/(TN+FP)")
sns.barplot(algorithms,Specificity)